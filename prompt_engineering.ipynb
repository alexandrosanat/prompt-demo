{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartynBonham-Yang/prompt-demo/blob/main/prompt_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e544a774",
      "metadata": {
        "id": "e544a774"
      },
      "source": [
        "<div style=\"display: flex; align-items: center;\">\n",
        "  <div style=\"flex: 1;\">\n",
        "    <img src=\"https://github.com/alexandrosanat/prompt-demo/blob/main/images/notebook.gif?raw=1\" alt=\"segment\" width=\"500\">\n",
        "  </div>\n",
        "  <div style=\"flex: 1;\">\n",
        "    <h2> Practical Prompt Engineering </h2>\n",
        "    <br>\n",
        "    <div>\n",
        "       In this tutorial, you‚Äôll learn how to:\n",
        "       <ul>\n",
        "        <li>Apply prompt engineering techniques to practical, real-world examples\n",
        "        <li>Tap into the power of roles in messages to go beyond using singular role prompts\n",
        "        <li>Use numbered steps, delimiters, few-shot prompting and other techniques to improve your results\n",
        "        <li>Understand and use chain-of-thought prompting to add more context\n",
        "      </ul>\n",
        "    <div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b248418",
      "metadata": {
        "id": "5b248418"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "‚ö†Ô∏è You can follow along by opening this notebook on google colab.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81a82fc7",
      "metadata": {
        "id": "81a82fc7"
      },
      "source": [
        "First let's install and then import all the libraries we're going to use..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42ebde61",
      "metadata": {
        "id": "42ebde61"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade openai\n",
        "!pip install jupyter-black"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "443b222a-e27f-4aca-8fb5-7588ea7d3ab4",
      "metadata": {
        "id": "443b222a-e27f-4aca-8fb5-7588ea7d3ab4",
        "outputId": "8f547ea3-8cc9-4cfb-9527-21efa98b8baf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
              "                (function() {\n",
              "                    if (window.IPython === undefined) {\n",
              "                        return\n",
              "                    }\n",
              "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
              "                        \"jupyter_black in a non-lab notebook with \" +\n",
              "                        \"`is_lab=True`. Please double check, and if \" +\n",
              "                        \"loading with `%load_ext` please review the README!\"\n",
              "                    console.log(msg)\n",
              "                    alert(msg)\n",
              "                })()\n",
              "                </script>\n",
              "                "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os, json\n",
        "import openai\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "%load_ext jupyter_black"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b099b72",
      "metadata": {
        "id": "6b099b72"
      },
      "source": [
        "and retrieve our OpenAI API key..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "767f9581",
      "metadata": {
        "id": "767f9581"
      },
      "outputs": [],
      "source": [
        "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3f116c3",
      "metadata": {
        "id": "a3f116c3"
      },
      "source": [
        "## Chat Completions API\n",
        "Chat models like GPT take a list of messages as input and return a model-generated message as output.\n",
        "\n",
        "Chat completion API [documentation](https://platform.openai.com/docs/guides/gpt):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI(api_key = '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')"
      ],
      "metadata": {
        "id": "yPZam9s8M6By"
      },
      "id": "yPZam9s8M6By",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0873da5e",
      "metadata": {
        "id": "0873da5e"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efa47359",
      "metadata": {
        "id": "efa47359"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "<b>Tip: </b>The 3 types of roles you can use are: <br>\n",
        "\n",
        "<ul>\n",
        "<li> <b>System role:</b> Allows you to specify the way the model answers questions. <br>\n",
        "    Typically we can use it to determine what <b>role</b> the AI should play and how it should behave generally. <br>\n",
        "\n",
        "<li> <b>User role:</b> Equivalent to the queries made by the user\n",
        "\n",
        "<li> <b>Assistant role:</b> The model‚Äôs responses\n",
        "</ul>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61416e86",
      "metadata": {
        "id": "61416e86"
      },
      "source": [
        "An example Chat Completions API [response](https://platform.openai.com/docs/guides/gpt/chat-completions-response-format) looks as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db256162",
      "metadata": {
        "id": "db256162"
      },
      "outputs": [],
      "source": [
        "response = {\n",
        "    \"choices\": [\n",
        "        {\n",
        "            \"finish_reason\": \"stop\",\n",
        "            \"index\": 0,\n",
        "            \"message\": {\n",
        "                \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\",\n",
        "                \"role\": \"assistant\",\n",
        "            },\n",
        "        }\n",
        "    ],\n",
        "    \"created\": 1677664795,\n",
        "    \"id\": \"chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW\",\n",
        "    \"model\": \"gpt-3.5-turbo-0613\",\n",
        "    \"object\": \"chat.completion\",\n",
        "    \"usage\": {\"completion_tokens\": 17, \"prompt_tokens\": 57, \"total_tokens\": 74},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec3068e4",
      "metadata": {
        "id": "ec3068e4"
      },
      "source": [
        "The assistant‚Äôs reply can be extracted with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2013a872",
      "metadata": {
        "id": "2013a872"
      },
      "outputs": [],
      "source": [
        "content = response[\"choices\"][0][\"message\"][\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content"
      ],
      "metadata": {
        "id": "5RahQDoeNYtP",
        "outputId": "dfe2fcde-8028-4459-c275-d3723ce5db7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "id": "5RahQDoeNYtP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Los Angeles Dodgers won the World Series in 2020.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1e16004",
      "metadata": {
        "id": "c1e16004"
      },
      "source": [
        "The next function allows us to send prompts to the model and get a response back. <br>\n",
        "\n",
        "We have specified the following model parameters:\n",
        "* Model: **GPT-3.5-turbo**\n",
        "* Temperature: **0**\n",
        "\n",
        "The function takes as inputs <u>prompt</u> and a <u>list of previous messages</u>, and returns a new list of messsages that include the prompt and the response from the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e9f6bf8",
      "metadata": {
        "id": "3e9f6bf8"
      },
      "outputs": [],
      "source": [
        "def get_completion_from_messages(\n",
        "    prompt, messages, model=\"gpt-3.5-turbo\", temperature=0\n",
        "):\n",
        "    if not isinstance(messages, list):\n",
        "        messages = [messages]\n",
        "\n",
        "    messages.append(prompt)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model, messages=messages, temperature=temperature\n",
        "    )\n",
        "\n",
        "    message = response.choices[0].message\n",
        "\n",
        "    print(message.content)\n",
        "\n",
        "    messages.append(message)\n",
        "\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7faadbe4",
      "metadata": {
        "id": "7faadbe4"
      },
      "source": [
        "Let's test it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad8dfb23",
      "metadata": {
        "scrolled": true,
        "id": "ad8dfb23",
        "outputId": "c29385d4-cde8-4bb5-bdac-a9cef01154b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your name is Alex.\n"
          ]
        }
      ],
      "source": [
        "# fmt: off\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"My name is Alex\"},\n",
        "    {\"role\": \"assistant\",\"content\": \"Nice to meet you, Alex! How can I assist you today?\",},\n",
        "]\n",
        "\n",
        "prompt = {\"role\": \"user\", \"content\": \"What is my name?\"}\n",
        "\n",
        "response = get_completion_from_messages(prompt, messages)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.choices[0].message.content"
      ],
      "metadata": {
        "id": "zkhVabIiPMuZ",
        "outputId": "68c22977-b250-429a-e4ec-8fa748b478c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "id": "zkhVabIiPMuZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello Alex! How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a6f4dfb",
      "metadata": {
        "id": "9a6f4dfb"
      },
      "source": [
        "# Ways to Engineer your Prompts ‚öôÔ∏è\n",
        "\n",
        "We will now look at a few different examples of how you can improve your promtps to become the utlimate prompt engineer!\n",
        "\n",
        "Let's explore the following techniques:\n",
        "\n",
        "<ul>\n",
        "<li> <b>Zero-shot prompting</b>\n",
        "<li> <b>Role prompting</b>\n",
        "<li> <b>Detail & Specificity</b>\n",
        "<li> <b>Few-shot prompting</b>\n",
        "<li> <b>Using delimiters</b>\n",
        "<li> <b>Chain-of-thought prompting (CoT)</b>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53b3ffd3",
      "metadata": {
        "id": "53b3ffd3"
      },
      "source": [
        "## 1. Zero-shot prompting üí¨\n",
        "\n",
        "First, we have a list of reviews written below. Our task is to summarise them using ChatGPT. <br> Let's see what we can do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a601ea55",
      "metadata": {
        "id": "a601ea55"
      },
      "outputs": [],
      "source": [
        "reviews = [\n",
        "    (\n",
        "        \"Date: December 15, 2021; Username: John123; Review: I am absolutely delighted \\\n",
        "    with this savings product! The interest rates are fantastic, and it has helped \\\n",
        "    me grow my savings significantly. Highly recommend!\"\n",
        "    ),\n",
        "    (\n",
        "        \"Date: November 28, 2021; Username: Sarah77; Review: I must say, I am quite \\\n",
        "    disappointed with this savings product. The promised returns were not as \\\n",
        "    impressive as advertised, and the fees associated with it added up quickly. \\\n",
        "    Not worth it.\"\n",
        "    ),\n",
        "    (\n",
        "        \"Date: January 5, 2022; Username: AlexSmith; Review: My opinion? I'm rather \\\n",
        "    indifferent about this savings product. It's just like any other basic savings \\\n",
        "    account out there. Nothing special, but it does the job.\"\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "987586de",
      "metadata": {
        "scrolled": true,
        "id": "987586de"
      },
      "outputs": [],
      "source": [
        "content = f\"\"\"Summarise these 3 reviews:{reviews}\"\"\"\n",
        "\n",
        "prompt = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": content,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33270a1a",
      "metadata": {
        "id": "33270a1a",
        "outputId": "fdf3a093-b8f2-4660-8ba6-d07c83a3ae71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review 1: John123 is extremely satisfied with the savings product, praising the fantastic interest rates and significant growth of their savings. They highly recommend it.\n",
            "\n",
            "Review 2: Sarah77 expresses disappointment with the savings product, stating that the promised returns were not as impressive as advertised and the associated fees accumulated quickly. They believe it is not worth it.\n",
            "\n",
            "Review 3: AlexSmith is indifferent about the savings product, considering it to be similar to any other basic savings account. They find nothing special about it but acknowledge that it fulfills its purpose.\n"
          ]
        }
      ],
      "source": [
        "messages = get_completion_from_messages(prompt, [])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6fcba1d",
      "metadata": {
        "id": "b6fcba1d"
      },
      "source": [
        "Great! It provided a summary of each review.\n",
        "\n",
        "What we just did is call **zero-shot prompting**, which is just a fancy way of saying that you‚Äôre asking a normal question or simply describing a task.\n",
        "\n",
        "But it's safe to say this is by no means useful enough yet to be used in a product.\n",
        "\n",
        "Let's now dive into the other prompt engineering techniques to improve our results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e69f702f",
      "metadata": {
        "id": "e69f702f"
      },
      "source": [
        "## 2. Role Prompting ü§ñ\n",
        "\n",
        "In the next example we are setting the **role** of the model via the system message.\n",
        "\n",
        "This is known as \"Role Prompting\", and is a general practice to help set the tone and context for the model's responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46c859b4",
      "metadata": {
        "id": "46c859b4"
      },
      "outputs": [],
      "source": [
        "system_message = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": (\n",
        "        \"\"\"\n",
        "        You are a Review Summariser.\n",
        "        Your job is to process reviews from customers,\n",
        "        and summarise them for analysis for the customer review team.\n",
        "        \"\"\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "messages = [system_message]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f2602b4",
      "metadata": {
        "id": "9f2602b4",
        "outputId": "bcc60472-9637-4b7a-fede-0e8052a5bfa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review 1: John123 is extremely satisfied with the savings product. They highlight the fantastic interest rates and how it has significantly helped them grow their savings. They highly recommend it.\n",
            "\n",
            "Review 2: Sarah77 is disappointed with the savings product. They mention that the promised returns were not as impressive as advertised and the associated fees added up quickly. They believe it is not worth it.\n",
            "\n",
            "Review 3: AlexSmith is indifferent about the savings product. They state that it is just like any other basic savings account and nothing special. However, they mention that it does the job.\n",
            "\n",
            "Overall, the reviews are mixed. While John123 highly recommends the product, Sarah77 is disappointed with it. AlexSmith has a neutral opinion, considering it to be an average savings account.\n"
          ]
        }
      ],
      "source": [
        "content = f\"\"\"Summarise these 3 reviews:{reviews}\"\"\"\n",
        "\n",
        "prompt = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": content,\n",
        "}\n",
        "\n",
        "messages = get_completion_from_messages(prompt, messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c39dba",
      "metadata": {
        "id": "a1c39dba"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "‚úÖ  By using a role prompt via the system message, the summary is more structured and much clearer to read for the user.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6870eef",
      "metadata": {
        "id": "b6870eef"
      },
      "source": [
        "Examples of this could include:\n",
        "- You are a financial advisor, qualified only to answer questions about finances\n",
        "- You are a helpful assistant that answer queries about a user's transactions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74d8b604",
      "metadata": {
        "id": "74d8b604"
      },
      "source": [
        "## 3. Detail and Specificity üîç\n",
        "\n",
        "It is important that when you prompt, you are specific and as clear as possible.\n",
        "\n",
        "Note that clear ‚â† short: the more detail you add, the better the model will understand how you want it to behave.\n",
        "\n",
        "In the next example, we'll try to add a category for each review: positive, neutral or negative:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e50eb70b",
      "metadata": {
        "id": "e50eb70b",
        "outputId": "0c225c26-de4e-4b44-f295-959e457cadbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review 1: Positive\n",
            "Review 2: Bad\n",
            "Review 3: Neutral\n"
          ]
        }
      ],
      "source": [
        "system_message = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"\"\"\n",
        "    You are a Review Summariser. \\\n",
        "    Your job is to process reviews from customers, \\\n",
        "    and summarise them for analysis for the customer review team.\n",
        "    Categorise each review as positive, neutral, or bad.\n",
        "    \"\"\",\n",
        "}\n",
        "content = f\"\"\"Summarise these 3 reviews:{reviews}\"\"\"\n",
        "\n",
        "prompt = {\"role\": \"user\", \"content\": content}\n",
        "\n",
        "messages = [system_message]\n",
        "messages = get_completion_from_messages(prompt, messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c67169f",
      "metadata": {
        "id": "5c67169f"
      },
      "source": [
        "As you can see, the model has only output the categories, and we're now missing all the other information.\n",
        "\n",
        "Let's be a bit more specific:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dfd7ddb",
      "metadata": {
        "scrolled": true,
        "id": "0dfd7ddb",
        "outputId": "8a4893de-a6a3-4dc0-8429-5ea9f8b49ac6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review 1:\n",
            "- Name: John123\n",
            "- Date: December 15, 2021\n",
            "- Sentiment: Positive\n",
            "- Summary: John123 is delighted with the savings product, praising the fantastic interest rates and significant growth of savings.\n",
            "\n",
            "Review 2:\n",
            "- Name: Sarah77\n",
            "- Date: November 28, 2021\n",
            "- Sentiment: Negative\n",
            "- Summary: Sarah77 is disappointed with the savings product, mentioning that the promised returns were not as impressive as advertised and the associated fees added up quickly.\n",
            "\n",
            "Review 3:\n",
            "- Name: AlexSmith\n",
            "- Date: January 5, 2022\n",
            "- Sentiment: Neutral\n",
            "- Summary: AlexSmith is indifferent about the savings product, stating that it is similar to any other basic savings account and nothing special, but it gets the job done.\n"
          ]
        }
      ],
      "source": [
        "system_message = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"\"\"\n",
        "    You are a Review Summariser. Your job is to process reviews\n",
        "    from customers and summarise them for analysis for the customer review team.\n",
        "\n",
        "    For each review output the name and date.\n",
        "    Also label each review as either 'Positive','Neutral', or 'Negative'.\n",
        "    Then provide a summary of key points in a single sentence.\n",
        "    \"\"\",\n",
        "}\n",
        "\n",
        "content = f\"\"\"Summarise these 3 reviews:{reviews}\"\"\"\n",
        "\n",
        "prompt = {\"role\": \"user\", \"content\": content}\n",
        "\n",
        "messages = [system_message]\n",
        "messages = get_completion_from_messages(prompt, messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90c63aab",
      "metadata": {
        "id": "90c63aab"
      },
      "source": [
        "Great! We now have all the information displayed appropriately for each review.\n",
        "\n",
        "It's even structured the data for us in a list!\n",
        "\n",
        "Let's see if we can further increase out specificity by numbering the steps as well as specifying the format that we want the model to use for our output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a45a6473",
      "metadata": {
        "id": "a45a6473"
      },
      "outputs": [],
      "source": [
        "system_message = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"\"\"\n",
        "    You are a Review Summariser. Your job is to process reviews\n",
        "    from customers and summarise them for analysis for the customer review team:\n",
        "\n",
        "    1. For each review output the name and date.\n",
        "    2. Also label each review as either 'Positive','Neutral', or 'Negative'.\n",
        "    3. Provide a summary of key points in a single sentence.\n",
        "\n",
        "    Output: When outputting this information, use the following structure:\n",
        "    - [Username]: name of customer who made review\n",
        "    - [Date]: date of review, written as dd/mm/yyyy\n",
        "    - [Sentiment]: review sentiment\n",
        "    - [Key points]: key points of each review in one\n",
        "    - [Summary]: a two sentense summary of the review\n",
        "    \"\"\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e011d72",
      "metadata": {
        "scrolled": true,
        "id": "7e011d72",
        "outputId": "c0c20eba-645e-4dc9-e236-af3292590e69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"reviews\": [\n",
            "        {\n",
            "            \"Username\": \"John123\",\n",
            "            \"Date\": \"15/12/2021\",\n",
            "            \"Sentiment\": \"Positive\",\n",
            "            \"Key points\": \"Delighted with the savings product, fantastic interest rates, helped grow savings significantly\",\n",
            "            \"Summary\": \"John123 is absolutely delighted with this savings product, praising the fantastic interest rates and significant growth of savings.\"\n",
            "        },\n",
            "        {\n",
            "            \"Username\": \"Sarah77\",\n",
            "            \"Date\": \"28/11/2021\",\n",
            "            \"Sentiment\": \"Negative\",\n",
            "            \"Key points\": \"Disappointed with the savings product, promised returns not as impressive as advertised, fees added up quickly\",\n",
            "            \"Summary\": \"Sarah77 is quite disappointed with this savings product, expressing dissatisfaction with the promised returns and the accumulation of fees.\"\n",
            "        },\n",
            "        {\n",
            "            \"Username\": \"AlexSmith\",\n",
            "            \"Date\": \"05/01/2022\",\n",
            "            \"Sentiment\": \"Neutral\",\n",
            "            \"Key points\": \"Indifferent about the savings product, similar to any other basic savings account\",\n",
            "            \"Summary\": \"AlexSmith is rather indifferent about this savings product, considering it to be just like any other basic savings account.\"\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "content = f\"\"\"Summarise these 3 reviews:{reviews} and return your response as a JSON\"\"\"\n",
        "\n",
        "prompt = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": content,\n",
        "}\n",
        "\n",
        "messages = [system_message]\n",
        "messages = get_completion_from_messages(prompt, messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f69a1e7a",
      "metadata": {
        "id": "f69a1e7a"
      },
      "source": [
        "As you can see, we get the output in JSON format.\n",
        "\n",
        "This is quite powerful: when we structure our output as we have done in the previous section, the model can easily convert this into a structured format as shown.\n",
        "\n",
        "We could now parse the model's output and use it in our product."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3b98004",
      "metadata": {
        "id": "d3b98004"
      },
      "source": [
        "## 4. Few-shot prompting üìÑ\n",
        "\n",
        "Few-shot prompting is a prompt engineering technique where you provide example tasks and their expected outputs in your prompt.\n",
        "\n",
        "So, instead of just describing the task you also provide examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb32539f",
      "metadata": {
        "id": "fb32539f"
      },
      "outputs": [],
      "source": [
        "system_message = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"\"\"\n",
        "    You are a Review Summariser. Your job is to process reviews\n",
        "    from customers and summarise them for analysis for the customer review team.\n",
        "\n",
        "    For each review output the name and date.\n",
        "    Also label each review as either 'Positive','Neutral', or 'Negative'.\n",
        "    Then provide a summary of key points in a single sentence.\n",
        "\n",
        "    Example 1:\n",
        "    \"Date: April 1, 2022\\nUsername: JaneDoe\\nReview: I'm really not happy with this savings product. \\\n",
        "    The interest rates are lower than promised and the customer service is lacking. I wouldn't recommend it.\",\n",
        "\n",
        "    Output 1:\n",
        "    - username: \"JaneDoe\"\n",
        "    - review_date: \"01/04/2022\"\n",
        "    - sentiment: \"negative\"\n",
        "    - key_points: \"Lower interest rates than promised, lacking customer service\"\n",
        "    - summary: JaneDoe is unhappy with the savings product, stating that the interest \\\n",
        "    rates are lower than promised and the customer service is lacking.\n",
        "    \"\"\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4470645b",
      "metadata": {
        "id": "4470645b",
        "outputId": "a4338ed8-4c96-418f-9078-e9f0d8ada2c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- username: \"John123\"\n",
            "- review_date: \"15/12/2021\"\n",
            "- sentiment: \"positive\"\n",
            "- key_points: \"Fantastic interest rates, significant savings growth\"\n",
            "- summary: John123 is delighted with the savings product, praising the fantastic interest rates and significant savings growth.\n",
            "\n",
            "- username: \"Sarah77\"\n",
            "- review_date: \"28/11/2021\"\n",
            "- sentiment: \"negative\"\n",
            "- key_points: \"Disappointing returns, high fees\"\n",
            "- summary: Sarah77 is disappointed with the savings product, expressing dissatisfaction with the returns not meeting expectations and the accumulation of high fees.\n",
            "\n",
            "- username: \"AlexSmith\"\n",
            "- review_date: \"05/01/2022\"\n",
            "- sentiment: \"neutral\"\n",
            "- key_points: \"Indifferent, basic savings account\"\n",
            "- summary: AlexSmith is indifferent about the savings product, describing it as a basic savings account without any standout features.\n"
          ]
        }
      ],
      "source": [
        "content = f\"\"\"Summarise these 3 reviews:{reviews}\"\"\"\n",
        "\n",
        "prompt = {\"role\": \"user\", \"content\": content}\n",
        "\n",
        "messages = [system_message]\n",
        "messages = get_completion_from_messages(prompt, messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ea24de1",
      "metadata": {
        "id": "3ea24de1"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "‚úÖ  Few-shot prompting can be really useful when we're asking the model to deal with input that it has never seen before. This is most powerful when we use a representative number of examples.\n",
        "\n",
        "For example, we might be dealing with a text categorisation scenario.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab03d091",
      "metadata": {
        "id": "ab03d091"
      },
      "source": [
        "## 5. Using Delimiters üîñ\n",
        "\n",
        "Delimiters can help to separate the content and examples from the task description. They can also make it possible to refer to specific parts of your prompt at a later point in the prompt. A delimiter can be any sequence of characters that usually wouldn‚Äôt appear together, for example:\n",
        "\n",
        "* \\>>>>>\n",
        "* \\====\n",
        "* \\####"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61744353",
      "metadata": {
        "id": "61744353"
      },
      "source": [
        "The number of characters that you use doesn‚Äôt matter too much, as long as you make sure that the sequence is relatively unique, otherwise this might confuse the model. Additionally, you can add labels just before or just after the delimiters:\n",
        "\n",
        "* START CONTENT>>>>> content <<<<<END CONTENT\n",
        "* \\==== START content END ====\n",
        "* \\#### START EXAMPLES examples #### END EXAMPLES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b94c27bc",
      "metadata": {
        "id": "b94c27bc"
      },
      "outputs": [],
      "source": [
        "system_message = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"\"\"\n",
        "    You are a Review Summariser. Your job is to process reviews\n",
        "    from customers and summarise them for analysis for the customer review team.\n",
        "\n",
        "    For each review output the name and date.\n",
        "    Also label each review as either 'Positive','Neutral', or 'Negative'.\n",
        "    Then provide a summary of key points in a single sentence.\n",
        "\n",
        "    #### START EXAMPLES ####\n",
        "\n",
        "    ------ Example Inputs ------\n",
        "    \"Date: April 1, 2022\\nUsername: JaneDoe\\nReview: I'm really not happy with this savings product. \\\n",
        "    The interest rates are lower than promised and the customer service is lacking. I wouldn't recommend it.\",\n",
        "\n",
        "    ------ Example Outputs ------\n",
        "    - username: \"JaneDoe\"\n",
        "    - review_date: \"01/04/2022\"\n",
        "    - sentiment: \"negative\"\n",
        "    - key_points: \"Lower interest rates than promised, lacking customer service\"\n",
        "    - summary: JaneDoe is unhappy with the savings product, stating that the interest \\\n",
        "    rates are lower than promised and the customer service is lacking.\n",
        "\n",
        "    #### END EXAMPLES ####\n",
        "\n",
        "    \"\"\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73a5387f",
      "metadata": {
        "scrolled": true,
        "id": "73a5387f",
        "outputId": "2a486e58-fdfa-44d2-c4a4-1908b133e5f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- username: \"John123\"\n",
            "  - review_date: \"15/12/2021\"\n",
            "  - sentiment: \"positive\"\n",
            "  - key_points: \"Fantastic interest rates, helped grow savings significantly\"\n",
            "  - summary: John123 is delighted with the savings product, praising the fantastic interest rates and how it has significantly helped grow their savings.\n",
            "\n",
            "- username: \"Sarah77\"\n",
            "  - review_date: \"28/11/2021\"\n",
            "  - sentiment: \"negative\"\n",
            "  - key_points: \"Promised returns not as impressive as advertised, fees added up quickly\"\n",
            "  - summary: Sarah77 is disappointed with the savings product, expressing that the promised returns were not as impressive as advertised and the fees associated with it added up quickly.\n",
            "\n",
            "- username: \"AlexSmith\"\n",
            "  - review_date: \"05/01/2022\"\n",
            "  - sentiment: \"neutral\"\n",
            "  - key_points: \"Indifferent, just like any other basic savings account\"\n",
            "  - summary: AlexSmith is indifferent about the savings product, stating that it is just like any other basic savings account out there and nothing special, but it does the job.\n"
          ]
        }
      ],
      "source": [
        "content = f\"\"\"Summarise these 3 reviews:{reviews}\"\"\"\n",
        "\n",
        "prompt = {\"role\": \"user\", \"content\": content}\n",
        "\n",
        "messages = [system_message]\n",
        "messages = get_completion_from_messages(prompt, messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24a8a75a",
      "metadata": {
        "id": "24a8a75a"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "‚úÖ  Splitting the prompt into sections becomes increasingly important as your prompt grows and multiple instructinos are passed to the model!\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63095780",
      "metadata": {
        "id": "63095780"
      },
      "source": [
        "# Chain-of-thought prompting (CoT) üí≠\n",
        "\n",
        "To apply CoT, you prompt the model to generate intermediate results that then become part of the prompt in a second request. The increased context makes it more likely that the model will arrive at a useful output.\n",
        "\n",
        "The smallest form of CoT prompting is **zero-shot CoT**, where you literally ask the model to *think step by step*. <br> This approach yields impressive results for mathematical tasks that LLMs otherwise often solve incorrectly.\n",
        "\n",
        "More commonly, chain-of-thought operations are technically split into two stages:\n",
        "\n",
        "- *Reasoning extraction*, where the model generates the increased context\n",
        "- *Answer extraction*, where the model uses the increased context to generate the answer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98e89fb1",
      "metadata": {
        "id": "98e89fb1"
      },
      "source": [
        "# Zero-shot CoT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe4407b2",
      "metadata": {
        "id": "fe4407b2",
        "outputId": "5ff00dce-7286-40ac-b164-70a96118c2ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Bought 10 apples.\n",
            "Step 2: Gave 2 apples to the neighbor. Remaining: 10 - 2 = 8 apples.\n",
            "Step 3: Gave 2 apples to the repairman. Remaining: 8 - 2 = 6 apples.\n",
            "Step 4: Bought 5 more apples. Total: 6 + 5 = 11 apples.\n",
            "Step 5: Ate 1 apple. Remaining: 11 - 1 = 10 apples.\n",
            "\n",
            "Therefore, you remained with 10 apples.\n"
          ]
        }
      ],
      "source": [
        "content = f\"\"\"\n",
        "I went to the market and bought 10 apples.\n",
        "I gave 2 apples to the neighbor and 2 to the repairman.\n",
        "I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
        "Let's think step by step.\n",
        "\"\"\"\n",
        "\n",
        "prompt = {\"role\": \"user\", \"content\": content}\n",
        "\n",
        "messages = []\n",
        "messages = get_completion_from_messages(prompt, messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90b8eb3f",
      "metadata": {
        "id": "90b8eb3f"
      },
      "source": [
        "# Summary üî•\n",
        "\n",
        "We have looked at all the main different techniques that you can use to engineer your prompts!\n",
        "\n",
        "<ul>\n",
        "<li> <b>Zero-shot prompting:</b> Asking the language model a normal question without any additional context\n",
        "<li> <b>Role prompting:</b> Specifying how the model will answer the questions\n",
        "<li> <b>Detail & Specificity:</b> Breaking down a complex prompt into a series of small, specific steps\n",
        "<li> <b>Few-shot prompting:</b> Conditioning the model on a few examples to boost its performance\n",
        "<li> <b>Using delimiters:</b> Adding special tokens or phrases to provide structure and instructions to the model\n",
        "<li> <b>Chain-of-thought prompting:</b> Prompt the model to generate intermediate results that then become part of the prompt in a second request\n",
        "\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edc45df1",
      "metadata": {
        "id": "edc45df1"
      },
      "source": [
        "# Prompt Engineering Tricks ü™Ñ\n",
        "\n",
        "<ul>\n",
        "<li> As the number of instructions increases, the model tends to <u>forget</u> some of them\n",
        "<li> Repeating instructions might help\n",
        "<li> Including instructions at the start vs at the end of the prompt will have a different effect\n",
        "<li> Logically <u>splitting the prompt</u> into sections helps a lot\n",
        "<li> Slightly tweaking the prompt might yield very different results\n",
        "<li> <u>Emphasising</u> specific instructions works, eg. \"ALWAYS include your thought process in your answer\"\n",
        "<li> Including <u>context</u> in the prompt helps reduce hallucinations\n",
        "<li> <u>Each use case is different</u>, so you will need to iterate to understand what works for yours\n",
        "\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aea77d79",
      "metadata": {
        "id": "aea77d79"
      },
      "source": [
        "# Bonus: Chain-of-thought prompting in functions üí•\n",
        "\n",
        "Chain of thought process is a powerful technique that has enabled LLMs to interact with functions and itegrate the provided context into their answers.\n",
        "\n",
        "Let's try to demonstrate this by answering the following question:\n",
        "\n",
        "*What is the weather in Edinburgh?*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95cc37e7",
      "metadata": {
        "id": "95cc37e7"
      },
      "source": [
        "First we need to modify our API call to enable function calling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ed170e1",
      "metadata": {
        "id": "8ed170e1"
      },
      "outputs": [],
      "source": [
        "def get_completion_from_messages_func(\n",
        "    prompt, messages, functions, model=\"gpt-3.5-turbo-0613\", temperature=0\n",
        "):\n",
        "    if not isinstance(messages, list):\n",
        "        messages = [messages]\n",
        "\n",
        "    messages.append(prompt)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        functions=functions,\n",
        "        function_call=\"auto\",\n",
        "        temperature=temperature,\n",
        "    )\n",
        "\n",
        "    message = response.choices[0].message\n",
        "\n",
        "    print(message)\n",
        "\n",
        "    messages.append(message)\n",
        "\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fecbc69b",
      "metadata": {
        "id": "fecbc69b"
      },
      "source": [
        "Now let's define our python API that returns the weather using a location as input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92f1f570",
      "metadata": {
        "id": "92f1f570"
      },
      "outputs": [],
      "source": [
        "def get_current_weather(location):\n",
        "    if location == \"Edinburgh\":\n",
        "        return {\"temperature\": 9, \"unit\": \"celsius\", \"description\": \"Sunny\"}\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "482d958b",
      "metadata": {
        "id": "482d958b"
      },
      "source": [
        "Remember! LLMs can only understand *language* so the only way for our model to have knowledge of this function is if we describe it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59d4eaae",
      "metadata": {
        "id": "59d4eaae"
      },
      "outputs": [],
      "source": [
        "function = [\n",
        "    {\n",
        "        \"name\": \"get_current_weather\",\n",
        "        \"description\": \"Get the current weather in a given location\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"location\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The location/city eg. London\",\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"location\"],\n",
        "        },\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0475af18",
      "metadata": {
        "id": "0475af18"
      },
      "source": [
        "Let's now send our question to the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdc12552",
      "metadata": {
        "id": "fdc12552",
        "outputId": "845bca5e-6a67-4f31-f2be-ed5b25c3729b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"location\": \"Edinburgh\"\\n}', name='get_current_weather'), tool_calls=None)\n"
          ]
        }
      ],
      "source": [
        "content = f\"\"\"What is the weather in Edinburgh?\"\"\"\n",
        "\n",
        "prompt = {\"role\": \"user\", \"content\": content}\n",
        "\n",
        "messages = []\n",
        "\n",
        "messages = get_completion_from_messages_func(prompt, messages, function)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9549d8cc",
      "metadata": {
        "id": "9549d8cc"
      },
      "source": [
        "First we need to extract the function name and function arguments from the response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dd00f95",
      "metadata": {
        "id": "7dd00f95"
      },
      "outputs": [],
      "source": [
        "function_name = messages[-1].function_call.name\n",
        "function_args = json.loads(messages[-1].function_call.arguments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "100efad8",
      "metadata": {
        "id": "100efad8",
        "outputId": "1a0274d0-12bb-4b48-bfd8-3618260742d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function Name: get_current_weather\n",
            "Function Arguments: {'location': 'Edinburgh'}\n"
          ]
        }
      ],
      "source": [
        "print(\"Function Name:\", function_name)\n",
        "print(\"Function Arguments:\", function_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9e743d5",
      "metadata": {
        "id": "a9e743d5"
      },
      "source": [
        "Let's now call our actual python function to get the response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9e3d860",
      "metadata": {
        "id": "e9e3d860",
        "outputId": "ac0c8666-e9c8-47a7-af93-62bc69523fa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'temperature': 9, 'unit': 'celsius', 'description': 'Sunny'}\n"
          ]
        }
      ],
      "source": [
        "function_response = eval(function_name)(**function_args)\n",
        "\n",
        "print(function_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8c74317",
      "metadata": {
        "id": "f8c74317"
      },
      "source": [
        "**Note:**\n",
        "\n",
        "This: ```eval(function_name)(**function_args)```\n",
        "\n",
        "is equivalent to this: ```get_current_weather(location='Edinburgh')```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40e55fc8",
      "metadata": {
        "id": "40e55fc8"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "<b>Tip: </b>There is actually only more role that can be used with ONLY gpt-3.5-turbo-0613 and gpt-4-0613. <br>\n",
        "\n",
        "<ul>\n",
        "<li> <b>Function role:</b> Allows you to specify the response of a function that the model can use to answer the original question.\n",
        "</ul>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98c24fac",
      "metadata": {
        "id": "98c24fac"
      },
      "outputs": [],
      "source": [
        "new_message = {\n",
        "    \"role\": \"function\",\n",
        "    \"name\": function_name,\n",
        "    \"content\": str(function_response),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da56ef29",
      "metadata": {
        "id": "da56ef29"
      },
      "source": [
        "Let's finally get the response from our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "111c66ce",
      "metadata": {
        "id": "111c66ce",
        "outputId": "fb267b9f-7e01-4e3e-d211-fc264d1dc28d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-80b781496020>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_completion_from_messages_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-65-6b24806b5ff3>\u001b[0m in \u001b[0;36mget_completion_from_messages_func\u001b[0;34m(prompt, messages, functions, model, temperature)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 556\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    557\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         )\n\u001b[0;32m-> 1055\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 834\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    835\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0;31m# to completion before attempting to access the response text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    878\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"'content' is a required property - 'messages.1'\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
          ]
        }
      ],
      "source": [
        "messages = get_completion_from_messages_func(new_message, messages, function)"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}